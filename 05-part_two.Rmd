# Advanced: Testing code{#testing} 

From a coding point of view, testing covers the requirements spelled out in the **Verification** pillar of the QA framework. Note that testing does not overlap with the **Validation** QA pillar: it is concerned with code running properly rather than with the adequacy of model assumptions. Testing is a vast topic and it is beyond the scope of this guide to explore it in depth. Here we concentrate on broad principles corresponding to incrementally thorough testing. These ways of testing include:

- console tests 
- in-script testing
- formalised testing procedures, such as unit testing (tests the different components or "units" of code using tools such as `testthat` in R)

## Console tests
Testing code within the R console is not recommended as it leaves no trace or records of what has been checked.

## In-script testing
In-script testing is sufficient to facilitate QA in most cases, although tests should always be accompanied by brief comments signalling their presence and indicating what they are evaluating and what the expected output should be. For a quality assurer, running these tests ensures that the entire analytical workflow is reproducible. In R there are many ways to perform in-script checks, from `print()` statements, to `base` functions `stop()` and `stopfinot()`, to functions from dedicated packages such as `assertthat` or `testit`. Examples are shown below. 

```{r stopifnot, eval = FALSE, class.source = "badCode"}

# This code snippet creates a function that
# returns the maximum value of a vector
# of numbers. The function contains 
# an in-script test checking that
# its input is numerical. If the condition
# is not met, execution is halted
get_max <- function(x) {
  stopifnot(is.numeric(x))
  max(x)
}

get_max(c(1:10))
# returns 10
get_max(c("a", "b", "c"))
# returns error message
# Error in get_max(c("a", "b", "c")) : is.numeric(x) is not TRUE


```


## Formalised testing
Beyond in-script tests, an analyst may decide to take a more formal approach to testing if the project warrants it. Software development has dedicated testing professionals (called "testers") and there is plenty of literature about software testing on the internet. Much of this is transferable to code based analysis, but not all of it. There are times when it makes sense to make a distinction between a model (which is generally operated by a technical specialist) and an IT product (e.g. a website, which is used by a broader user base). Shiny apps sit somewhere between a model and an IT product so we'll include some specific comments about these.

There are often four types of software testing covered:

* Unit testing (tests the different components or "units" of code using tools such as `testthat` in R).
* Integration testing (tests the correct flow of data through the components).
* System testing (tests the overall outputs make sense).
* Acceptance testing (tests whether or not the work meets its objectives).

Software testing can make a distinction between functional and non-functional testing too. In the context of an analytical model, a functional test would be one which tests the model produces what its intended to whereas a non-functional test would cover things such as its run-time and interface (i.e. things which affect the experience of using it, but not the numbers that come out).

For more information, see [here](https://www.guru99.com/levels-of-testing.html) or use your favourite search engine to explore the topic.

As this is a chapter about testing code, we are only going to cover unit testing in more detail. The other testing aspects cover testing of wider areas of a software project than just the code, and other parts of the DfE QA framework cover how to handle these:

* Integration testing tests correct data flow, transformations and interactions between different model components.
* System testing relates to verification tests on the overall outputs.
* Acceptance testing relates to the validation pillar of the QA framework.

### Unit testing

Unit testing seeks to test every distinct component within your code to enable to you isolate specific areas that may be introducing errors. If you have written you code using functions, it is sometimes easier to think of this as testing each specific function.

One of the key principles of unit testing is that each unit (or function) should be tested independently of all the others and without dependency on any data sources that may change. The objective is to isolate the cause of any errors:

* If you test a function relying on output from previous functions and that test throws an error, you do not immediately know which of the functions (or units) caused the error and therefore it is not a unit test.
* If you test a function relying on data from a live source (e.g SQL), it is possible your data could change and cause you test to fail without your function (unit) having any issues so this is also not a unit test.

To write a good set of unit tests, you need to think about the different types of input (e.g. positive values, zero values) your function may receive, define some fixed inputs to cover these and the write your tests to ensure the different cases are handled correctly.

To implement unit tests, it is generally easiest to use a unit testing framework. These generally allow you to write tests in separate files and then run all tests at once and return success/failure feedback.

* The `testthat` package is a popular tool to write and run tests in R (more details [here](https://github.com/r-lib/testthat)).
* The `pytest` framework offers a similar resource for Python (more details [here](https://docs.pytest.org/en/latest/))

### testthat

The `testthat` package, maintained by the RStudio team, is commonly used for unit testing in R. Although most of its documentation and features focus on testing in the context of package writing, `testthat` can also be used with regular R projects.  There's a brief overview of unit testing below, but for a more detailed introduction see [here](https://katherinemwood.github.io/post/testthat/) or [here](https://www.r-bloggers.com/automated-testing-with-testthat-in-practice/) for a more in-depth overview of automated testing, including test driven development. 

In `testthat`, tests are called **expectations**. These spell out the expected behaviour of your function or unit of code. `testthat` provides many functions to define expectations - see details [here](http://r-pkgs.had.co.nz/tests.html). It is good practice to save tests relating to the different parts of your code in different files, all saved within a dedicated testing directory. Below is an example of testing a simple function.

Firstly we need to have a defined function to test:
```{r testing_function, eval=TRUE}
# Create a simple function to demonstrate testing approach
library(tibble)

# create a data frame of random numbers
create_random_numbers_dataframe <- function(n){
  tibble(some_numbers = runif(n))
}
```

Then we can defined some tests for it:
```{r testthat, eval = FALSE, warning=FALSE}
# This file is stored in a directory called "testing_directory" 
# Need the testthat package
library(testthat)

# Need to source our function
source("file_where_function_saved.R")

# Need to run our function to be able to test its impact
# Could also do this within tests if we wanted to test different cases
mydata <- create_random_numbers_dataframe(100)

# The first test checks that the data set
# "mydata" has a a single column and 100 rows
test_that('data dimensions correct', {
  expect_equal(ncol(mydata), 1)
  expect_equal(nrow(mydata), 100)
})

# The second test checks that the maximum value
# of the variable "some_numbers" does not exceed 1
test_that('no value greater than 1', {
  expect_lte(max(mydata$some_numbers), 1)
})

```

Expectations are tested using the `test_dir()` function, which takes the path to the testing directory as an argument:
```{r testing, message = FALSE, eval = FALSE, class.source = "badCode"}
# test expectations and examine outputs
# In this case, all tests are OK
test_dir("testing_directory")
```

`test_dir()` provides detailed outputs, including a time line of success/failure and any warnings that may have occurred.

### Testing Shiny apps

R Shiny apps may benefit from a further extended testing approach. They encompass not just some analysis, but also a User Interface and a Server to enable them to run. This makes them closer to a software development project than the average piece of modeling work.

Additional functionality is available within R to support testing of Shiny apps. Further details about this can be found [here](https://shiny.rstudio.com/articles/integration-testing.html).


### Test driven development